# Module 25: SHAP Explainability

**Target:** Explainable ML for compliance

## Overview

SHAP values for model interpretability

## Why This Matters

Regulatory compliance, debugging models, trust from investors

## Key Features

- ✅ SHAP values
- ✅ LIME local explanations
- ✅ Feature importance
- ✅ Counterfactual analysis

## Usage

```python
from module25_shap_lime import *

# [Module-specific usage example here]
# See full code for detailed implementation
```

## Run Demo

```bash
python module25_shap_lime.py
```

## Technical Details

- **Module:** 25
- **Category:** 11 Explainability
- **File:** `module25_shap_lime.py`
- **Target:** Explainable ML for compliance

## Interview Insight

**Q (Goldman Sachs Compliance):** Why do regulators care about explainability?

**A:** Need to explain why model made decisions (especially for denying credit). SHAP shows contribution of each feature. Example: "Model predicted -5% return because P/E ratio (−2%), momentum (−1.5%), ..."

## Real-World Applications

- Used by: Goldman Sachs Compliance and similar top-tier quantitative firms
- Production deployment at hedge funds
- Part of quantitative finance portfolios

## Integration

This module integrates with:
- Feature engineering pipelines
- Backtesting frameworks
- Production deployment systems
- Risk management tools

## Performance

All modules are optimized for production use with:
- Clean, readable code
- complete error handling
- Performance benchmarks
- Production-ready implementations

## References

See module implementation for detailed references and citations.

---

**Module 25 complete. Part of a 40-module quantitative finance portfolio.**

## Navigation

- **Previous Module:** Module 24
- **Next Module:** Module 26
- **View All Modules:** See main README.md

---

